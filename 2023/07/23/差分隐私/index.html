
<!DOCTYPE html>
<html lang="en" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>差分隐私 - Hexo</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Fechin,"> 
    <meta name="description" content="范数范数距离是一种衡量向量之间差异的方法，就像用简单的话来解释，你可以把它想象成衡量两个箭头有多远。假设一个箭头代表一个向量，它有长度和方向。范数距离就是看两个箭头的尾部放在一起，然后测量它们的尖端,"> 
    <meta name="author" content="FOW"> 
    <link rel="alternative" href="atom.xml" title="Hexo" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    
    
    
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="差分隐私 - Hexo"/>
    <meta name="twitter:description" content="范数范数距离是一种衡量向量之间差异的方法，就像用简单的话来解释，你可以把它想象成衡量两个箭头有多远。假设一个箭头代表一个向量，它有长度和方向。范数距离就是看两个箭头的尾部放在一起，然后测量它们的尖端,"/>
    
    
    
    
    <meta property="og:site_name" content="Hexo"/>
    <meta property="og:type" content="object"/>
    <meta property="og:title" content="差分隐私 - Hexo"/>
    <meta property="og:description" content="范数范数距离是一种衡量向量之间差异的方法，就像用简单的话来解释，你可以把它想象成衡量两个箭头有多远。假设一个箭头代表一个向量，它有长度和方向。范数距离就是看两个箭头的尾部放在一起，然后测量它们的尖端,"/>
    
<link rel="stylesheet" href="/css/diaspora.css">

<meta name="generator" content="Hexo 6.3.0"></head>

<body class="loading">
    <span id="config-title" style="display:none">Hexo</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="http://example.com"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle">差分隐私</h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">差分隐私</h1>
        <div class="stuff">
            <span>七月 23, 2023</span>
            
  <ul class="post-tags-list" itemprop="keywords"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/FATE/" rel="tag">FATE</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/" rel="tag">联邦学习</a></li></ul>


        </div>
        <div class="content markdown">
            <h4 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h4><p><strong>范数距离</strong>是一种衡量向量之间差异的方法，就像用简单的话来解释，你可以把它想象成衡量两个箭头有多远。假设一个箭头代表一个向量，它有长度和方向。范数距离就是看两个箭头的尾部放在一起，然后测量它们的尖端有多远，这样就可以知道这两个向量之间有多大的差异。如果箭头的尖端很近，那么范数距离就小，表示向量相似；如果箭头的尖端很远，范数距离就大，表示向量差异很大。</p>
<ol>
<li>L1范数（一阶范数）： L1范数是向量中各个元素绝对值之和。</li>
</ol>
<p>L1范数：∥x∥₁ &#x3D; |x₁| + |x₂| + … + |xₙ|</p>
<ol start="2">
<li>L2范数（二阶范数）： L2范数是向量中各个元素平方和的平方根，也称为欧几里德范数。</li>
</ol>
<p>L2范数：∥x∥₂ &#x3D; √(x₁² + x₂² + … + xₙ²)</p>
<ol start="3">
<li>L0范数（零范数）： L0范数是向量中非零元素的个数。</li>
</ol>
<p>L0范数：∥x∥₀ &#x3D; 非零元素的个数</p>
<ol start="4">
<li>L∞范数（无穷范数）： L∞范数是向量中绝对值最大的元素。</li>
</ol>
<p>L∞范数：∥x∥∞ &#x3D; max(|x₁|, |x₂|, …, |xₙ|)</p>
<h6 id="L1范数（一阶范数）："><a href="#L1范数（一阶范数）：" class="headerlink" title="L1范数（一阶范数）："></a>L1范数（一阶范数）：</h6><p>在机器学习中，L1范数经常用于正则化。正则化是一种通过在损失函数中引入模型参数的惩罚项来防止过拟合的技术。L1范数正则化倾向于使一些模型参数变为零，从而产生稀疏模型。这可以用于特征选择，通过将不重要的特征的系数降低甚至置零，从而帮助模型更好地泛化和减少噪音。</p>
<h6 id="L2范数（二阶范数）："><a href="#L2范数（二阶范数）：" class="headerlink" title="L2范数（二阶范数）："></a>L2范数（二阶范数）：</h6><p>L2范数也常用于正则化。与L1范数不同，L2范数正则化会使所有模型参数都变小，但很少变为零。这有助于平滑模型的系数，防止参数过大，从而减少过拟合。L2范数也用于损失函数中，例如在支持向量机（SVM）和神经网络的权重衰减中。</p>
<h6 id="L0范数（零范数）："><a href="#L0范数（零范数）：" class="headerlink" title="L0范数（零范数）："></a>L0范数（零范数）：</h6><p>L0范数在机器学习中很有用，因为它可以促使模型产生稀疏解。然而，计算L0范数非常困难，因为它需要计算非零元素的个数。因此，在实际应用中，L0范数往往被近似或通过其他方法来实现，如L1范数正则化。</p>
<h6 id="L∞范数（无穷范数）："><a href="#L∞范数（无穷范数）：" class="headerlink" title="L∞范数（无穷范数）："></a>L∞范数（无穷范数）：</h6><p>L∞范数用于衡量向量中最大的绝对值。在机器学习中，它可以用于异常值检测和异常点的剔除。此外，在图像处理中，L∞范数可以用于衡量像素值的变化，有助于检测图像中的边缘和纹理。</p>
<p>总之，这些范数在机器学习中有着广泛的应用，可以帮助优化模型、增强泛化能力、减少过拟合以及处理数据和特征。根据具体任务和需求，选择合适的范数可以提升模型的性能和稳定性。</p>
<h4 id="全局敏感度和局部敏感度的区别"><a href="#全局敏感度和局部敏感度的区别" class="headerlink" title="全局敏感度和局部敏感度的区别"></a>全局敏感度和局部敏感度的区别</h4><p><strong>全局敏感度（Global Sensitivity）：</strong></p>
<p>全局敏感度是指在数据集中任何两个相邻的数据记录之间，对敏感查询的响应差异的最大可能值。换句话说，全局敏感度是数据集中的最大敏感度，无论查询操作选择哪两个相邻的数据记录进行比较。全局敏感度对整个数据集进行了考虑，因此可以看作是数据处理操作对整个数据集的最大影响。</p>
<p><strong>局部敏感度（Local Sensitivity）：</strong></p>
<p>局部敏感度是指在数据集中某一个数据记录发生变化时，对敏感查询的响应变化的最大可能值。换句话说，局部敏感度是某一个数据记录的变化对查询响应的最大影响。局部敏感度考虑了每个数据记录的影响，因此它更加关注个别数据记录的隐私影响。</p>
<p>简而言之，全局敏感度关注整个数据集的最大影响，而局部敏感度关注单个数据记录的最大影响。在不同隐私保护技术中，选择使用哪种敏感度取决于问题的性质和需求。</p>
<h4 id="平滑敏感度"><a href="#平滑敏感度" class="headerlink" title="平滑敏感度"></a>平滑敏感度</h4><p>平滑敏感度（Smooth Sensitivity）是隐私保护领域中的一个概念，用于在差分隐私机制中衡量查询操作对隐私的影响。平滑敏感度是一种对全局敏感度的近似，它在一定程度上减少了数据集的噪音扰动，从而提高了隐私保护的效果。</p>
<p>在差分隐私中，噪音被添加到查询结果中以保护隐私。然而，为了达到隐私保护的目标，噪音的添加会导致查询结果的失真。平滑敏感度的目标是减少这种失真，同时保护隐私。平滑敏感度通过对数据集中的每个数据记录进行微小的变化，然后计算查询的响应差异，来近似全局敏感度。</p>
<p>具体来说，平滑敏感度会对数据集进行微小的修改，然后计算查询操作在修改后数据集上的响应差异。这些微小的修改会减少查询结果的变化，从而减少添加到结果中的噪音。平滑敏感度的思想是在数据集中引入一些扰动，以降低敏感度，并在随后的查询中产生较小的响应差异。</p>
<p>平滑敏感度是差分隐私中的一种技术，用于提高查询结果的质量和隐私保护的效果。它是在保护隐私和数据可用性之间的权衡中的一种方法。</p>
<h4 id="拉普拉斯机制"><a href="#拉普拉斯机制" class="headerlink" title="拉普拉斯机制"></a>拉普拉斯机制</h4><h5 id="拉普拉斯分布"><a href="#拉普拉斯分布" class="headerlink" title="拉普拉斯分布"></a>拉普拉斯分布</h5><p>1.小规模数据集： 拉普拉斯机制在小规模数据集上表现良好，特别是当数据集的大小相对较小，且噪声的影响相对较小时。对于小规模数据集的差分隐私应用，拉普拉斯机制是一个合适的选择。</p>
<p>2.频率查询： 当数据集需要进行频率查询（如计数、直方图等）时，拉普拉斯机制是适用的。它可以通过在查询结果中添加噪声，保护个体的隐私，同时仍然提供对查询结果的有意义的近似。</p>
<p>3.排名查询： 拉普拉斯机制在排名查询（如Top-k查询）中也很适用。例如，在查询最高销售额的商品时，可以使用拉普拉斯机制为查询结果添加噪声，以保护每个商品的销售额信息。</p>
<p>4.数值型数据： 当数据集包含数值型数据时，拉普拉斯机制的添加噪声方式能够比较自然地应用于查询。这使得它适用于许多实际的数据分析场景，如统计分析、数据挖掘等。</p>
<p>5.查询数量有限： 拉普拉斯机制在有限数量的查询场景中表现良好。当查询数量相对较少时，添加的噪声对数据分析结果的影响较小，可以在隐私保护和数据可用性之间取得较好的平衡。</p>
<h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><p>学习率（Learning Rate）是深度学习中一个重要的超参数，用于控制模型参数在训练过程中的更新步长或速度。学习率决定了模型参数在每一轮训练中应该沿着损失函数梯度的反方向更新多远。选择合适的学习率对于训练神经网络和其他机器学习模型至关重要，因为学习率的不当选择可能导致训练过程不稳定、收敛缓慢或根本不收敛。</p>
<p>以下是学习率的一些关键概念和注意事项：</p>
<ol>
<li><strong>学习率的大小</strong>：学习率通常是一个小正数，典型取值范围在0.1到0.0001之间。学习率过大可能导致训练过程不稳定，甚至发散；学习率过小可能导致训练过程收敛缓慢。</li>
<li><strong>学习率的调度</strong>：在训练过程中，可以使用学习率调度策略，逐渐降低学习率，以帮助模型在接近收敛时更精确地收敛。常见的学习率调度策略包括学习率衰减（learning rate decay）和学习率退火（learning rate annealing）。</li>
<li><strong>自适应学习率</strong>：一些优化算法，如Adam、RMSprop和Adagrad，具有自适应学习率的特性，它们可以自动调整每个参数的学习率，以加速收敛并提高性能。</li>
<li><strong>超参数调优</strong>：选择合适的学习率通常需要进行超参数调优（Hyperparameter Tuning），可以通过网格搜索、随机搜索或更高级的优化算法来找到最佳的学习率。</li>
<li><strong>观察损失曲线</strong>：在训练过程中，观察损失曲线，以确保模型在训练过程中是否收敛。如果损失曲线不稳定，可能是学习率太大或太小的原因。</li>
<li><strong>应对梯度爆炸和梯度消失</strong>：学习率的选择也可以影响梯度爆炸和梯度消失问题。较小的学习率可能有助于减少梯度爆炸，但也可能导致梯度消失问题。在这种情况下，可以考虑使用梯度裁剪或其他技巧来应对这些问题。</li>
</ol>
<p>总之，学习率是深度学习训练中需要仔细调整的关键超参数之一。选择合适的学习率可以帮助模型更快地收敛到最优解，提高训练效率和性能。因此，在深度学习项目中，通常需要进行学习率的调优和实验以找到最佳的学习率设置。</p>
<h4 id="client-drift"><a href="#client-drift" class="headerlink" title="client-drift"></a>client-drift</h4><p>“Client Drift” 是指在联邦学习中，参与联邦学习的客户端（Client）在训练过程中的性能和数据分布发生不稳定或漂移的情况。这种漂移可能会对联邦学习系统的性能产生负面影响，因此需要仔细监控和管理。</p>
<p>以下是一些关于客户端漂移（Client Drift）的重要信息：</p>
<ol>
<li><strong>性能漂移：</strong> 客户端性能漂移指的是在联邦学习中，参与者的本地模型性能发生不稳定变化的情况。这可能是由于客户端的计算能力、网络连接、本地数据质量或其他因素导致的。性能漂移可能导致模型更新的不稳定性，从而影响全局模型的性能。</li>
<li><strong>数据分布漂移：</strong> 客户端数据分布漂移是指参与者的本地数据分布在训练过程中发生变化。这可能是因为客户端的数据采样方式或数据更新频率不稳定，导致不同客户端提供的数据分布不一致。数据分布漂移可能会导致模型在不同客户端之间的性能差异。</li>
<li><strong>监控和管理：</strong> 为了应对客户端漂移，联邦学习系统需要具有监控和管理机制。这可能包括监测客户端的性能指标、检测数据分布变化、自动重新选择客户端或动态调整训练策略等措施。</li>
<li><strong>客户端选择：</strong> 在一些联邦学习系统中，可以采用智能的客户端选择策略，以确保不稳定的客户端不会对全局模型产生过大的负面影响。这可能涉及排除性能较差的客户端或优先选择性能稳定的客户端。</li>
<li><strong>通信和同步：</strong> 通信和同步策略也可以影响客户端漂移。采用更频繁的通信和同步可能有助于减轻漂移的影响，但会增加通信开销。</li>
</ol>
<p>客户端漂移是联邦学习中需要关注的一个重要问题，特别是在分布式和异构数据环境中。有效的监控、管理和应对客户端漂移可以提高联邦学习系统的稳定性和性能。</p>
<h4 id="epoch"><a href="#epoch" class="headerlink" title="epoch"></a>epoch</h4><p>在机器学习领域，<strong>epoch（时期）</strong> 是指整个训练数据集被模型使用一次的过程。在训练神经网络等机器学习模型时，通常将数据集分成小批次（batches）进行训练，每个小批次包含一定数量的样本。</p>
<p>一个 epoch 完成时，意味着模型已经看过整个训练数据集一次，通过反向传播和优化算法来更新模型的权重和参数。整个训练过程可能会包含多个 epoch，以确保模型能够更好地学习训练数据的特征，提高泛化能力。</p>
<p>在深度学习中，模型的训练通常需要多个 epoch，因为通过多次迭代，模型能够逐渐调整权重，降低训练误差，提高对新数据的泛化能力。epoch 的数量通常是训练过程中的一个超参数，需要通过实验和验证集的表现来调整。增加 epoch 数量可能有助于模型更好地学习数据的特征，但也可能导致过拟合（overfitting），尤其是在训练数据量较小的情况下。</p>
<h4 id="泛化误差"><a href="#泛化误差" class="headerlink" title="泛化误差"></a>泛化误差</h4><p><strong>泛化误差（Generalization Error）</strong>  是机器学习中一个关键的概念，指的是模型在面对未曾见过的新数据时的性能表现。简而言之，泛化误差是模型在实际应用中的性能，它衡量了模型对于新数据的适应能力。</p>
<p>在机器学习中，训练一个模型的目标是使其在训练数据上表现良好，同时也要确保其在未见过的数据上能够有较好的表现。泛化误差的计算方式通常是通过对一个独立的测试数据集进行评估来实现的。</p>
<p>泛化误差受到过拟合和欠拟合的影响：</p>
<ol>
<li><strong>过拟合（Overfitting）：</strong> 当模型过于复杂，以至于在训练数据上表现非常好，但在新数据上表现较差时，就会发生过拟合。过拟合时，模型可能学到了训练数据中的噪声和细节，而无法很好地泛化到新数据。</li>
<li><strong>欠拟合（Underfitting）：</strong> 相反，如果模型过于简单，以至于无法捕捉训练数据中的关键模式和信息，那么模型在训练和新数据上都可能表现不佳。</li>
</ol>
<p>为了找到合适的模型，需要在训练过程中进行调整，以平衡模型的复杂性和泛化性能。通过使用验证集来选择合适的模型超参数，可以帮助防止过拟合和欠拟合，从而提高泛化能力。</p>
<h4 id="数学符号"><a href="#数学符号" class="headerlink" title="数学符号"></a>数学符号</h4><h5 id="∇（nabla）"><a href="#∇（nabla）" class="headerlink" title="∇（nabla）"></a>∇（nabla）</h5><p>是一个数学符号，通常用于表示梯度（Gradient）或散度（Divergence）运算。这个符号在矢量分析、微积分、物理学等领域中广泛使用。它有不同的用途，具体取决于上下文。</p>
<p>以下是∇的两个主要用法：</p>
<ol>
<li><p><strong>梯度 (∇ 的用法)：</strong> 在向量分析中，∇ 通常表示梯度运算。对于一个标量函数 f(x, y, z)（其中 x、y、z 是空间坐标），∇f 表示这个函数在空间中的梯度向量，它包含了函数在各个方向上的变化率。梯度运算通常用于优化问题和描述函数在空间中的变化。</p>
<p>例如，如果 f(x, y, z) 表示三维空间中的温度分布，∇f 将表示温度分布在每个点上的温度变化方向和速率。∇f 的方向指向最快的温度升高方向，而∇f 的大小表示了温度上升的速率。</p>
</li>
<li><p><strong>散度 (∇·的用法)：</strong> 在向量分析中，∇·（读作”nabla dot”）通常表示散度运算。对于一个矢量场（向量函数） F(x, y, z) &#x3D; [P(x, y, z), Q(x, y, z), R(x, y, z)]，∇·F 表示这个矢量场的散度，它用于描述矢量场中的流量密度。</p>
<p>例如，在流体动力学中，矢量场可以表示流体速度场，∇·F 描述了在某一点上的流体的收缩或扩散程度。正散度表示流体在该点汇聚，负散度表示流体在该点散开。</p>
</li>
</ol>
<p>总之，∇ 是一个重要的数学符号，它在矢量分析和微积分中用于表示梯度和散度运算。这些运算在物理学、工程学、计算机图形学等许多领域中都具有广泛的应用。</p>
<h4 id="失真度"><a href="#失真度" class="headerlink" title="失真度"></a>失真度</h4><p>在不同的上下文中，“失真度”可以有不同的含义。在机器学习和信息论中，通常是指某种度量，用于衡量模型或信号的输出与真实值之间的差异或失真程度。以下是一些可能的解释：</p>
<ol>
<li><strong>信号处理和信息论：</strong> 在通信领域，失真度是指由于信号传输过程中引入的信号变形或畸变。失真度的测量可以包括信号失真的程度和形式，例如峰值失真、均方根失真等。</li>
<li><strong>图像处理：</strong> 在图像处理中，失真度通常指的是图像质量的损失程度。图像的失真可以由于压缩、降噪等操作引起。失真度的度量可能涉及到图像的结构、颜色和清晰度等方面。</li>
<li><strong>音频处理：</strong> 在音频处理中，失真度指的是音频信号的失真程度，例如由于压缩算法或噪声引起的音频质量损失。</li>
<li><strong>机器学习中的失真：</strong> 在机器学习领域，失真度可能指的是模型对输入数据的学习误差或预测误差。例如，在回归问题中，可以通过比较模型的预测输出和实际目标值来度量失真度。</li>
</ol>
<p>具体来说，失真度通常是通过某种度量标准（如均方误差、交叉熵等）来衡量的，它提供了一种量化模型或系统输出与真实值之间差异的方式。在具体的应用中，失真度的定义和度量方式可能会有所不同。</p>
<h4 id="联邦学习常见超参数"><a href="#联邦学习常见超参数" class="headerlink" title="联邦学习常见超参数"></a>联邦学习常见超参数</h4><ol>
<li><strong>学习率（Learning Rate）：</strong> 控制模型参数更新的步长。过大的学习率可能导致模型不稳定，而过小的学习率可能使模型收敛缓慢。</li>
<li><strong>本地训练轮次（Local Epochs）：</strong> 每个参与者在本地训练中运行的轮次数。本地训练轮次的选择会影响模型在每个参与者上的收敛速度。</li>
<li><strong>批量大小（Batch Size）：</strong> 用于本地训练的每个小批量样本的数量。批量大小的选择可以影响模型训练的速度和稳定性。</li>
<li><strong>通信轮次（Communication Rounds）：</strong> 联邦学习中参与者之间通信的轮次。每个通信轮次包括本地训练和全局模型聚合。</li>
<li><strong>全局模型聚合方式：</strong> 定义如何聚合来自不同参与者的模型更新，常见的方法包括FedAvg（联邦平均）和FedProx（带罚项的联邦平均）等。</li>
<li><strong>差分隐私参数：</strong> 如果在联邦学习中使用差分隐私，需要设置差分隐私的参数，如隐私预算（privacy budget）、噪声尺度等。</li>
<li><strong>权重衰减（Weight Decay）：</strong> 控制正则化项的强度，用于防止模型过拟合。</li>
</ol>
<p>权重衰减（Weight Decay）是一种正则化技术，用于防止模型在训练过程中过度拟合训练数据。它通过在损失函数中引入一个额外的惩罚项，惩罚模型的权重参数，使得模型更趋向于学习简单的模式，而不是过度依赖训练数据的噪声。</p>
<p>在权重衰减中，正则化项被添加到损失函数中，通常采用下面的形式：<br>$$<br>L_{total} &#x3D; L_{data} + \lambda \cdot weight_decay<br>$$<br>其中：</p>
<ul>
<li>$$L_{total}$$是包括正则化项的总损失函数.</li>
<li>$$\lambda$$是权重衰减的超参数，控制正则化项的强度。</li>
<li>$$weight_decay$$ 是权重衰减项，通常定义为所有权重参数的平方和或平方范数。</li>
</ul>
<p>权重衰减的作用是通过降低权重参数的数值来防止模型过度拟合。正则化项的加入使得优化算法在更新模型参数时更倾向于选择较小的权重值，从而抑制过度复杂的模型。这有助于提高模型在未见过的数据上的泛化能力。</p>
<p>在深度学习中，常见的优化器，如SGD（随机梯度下降）和Adam，通常都支持权重衰减。在训练过程中，权重衰减的超参数 �<em>λ</em> 可以通过交叉验证等方法进行调整，以找到最佳的正则化强度。</p>
<ol>
<li><strong>本地模型更新次数：</strong> 控制每个参与者在每个通信轮次中进行本地模型更新的次数。</li>
<li><strong>模型架构：</strong> 包括模型的层数、隐藏单元数量等。不同的任务可能需要不同的模型架构。</li>
<li><strong>优化器（Optimizer）：</strong> 选择用于模型参数优化的优化算法，例如SGD（随机梯度下降）或Adam。</li>
<li><strong>损失函数（Loss Function）：</strong> 定义了模型在训练过程中的优化目标。</li>
</ol>
<p>这些超参数的选择通常需要通过实验和调优来确定，因为最佳的超参数取决于具体的联邦学习任务、数据分布和模型架构等因素。</p>
<h4 id="附"><a href="#附" class="headerlink" title="附"></a>附</h4><p>Dwork 附录A篇<br>高斯机制</p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title="0" data-url="http://link.hhtjim.com/163/425570952.mp3"></li>
                        
                    
                        
                            <li title="1" data-url="http://link.hhtjim.com/163/425570952.mp3"></li>
                        
                    
                </ul>
            
        </div>
        
    <div id="gitalk-container" class="comment link"
		data-enable="false"
        data-ae="false"
        data-ci=""
        data-cs=""
        data-r=""
        data-o=""
        data-a=""
        data-d="false"
    >查看评论</div>


    </div>
    
</div>


    </div>
</div>
</body>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>






</html>
